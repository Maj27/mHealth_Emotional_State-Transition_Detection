{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hyperparameter tuning of All classifiers for emotional state detection\n",
    "* 6 fold cross validation with grid-search\n",
    "* Multiclass classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics   \n",
    "from sklearn.feature_selection import SelectFromModel,RFECV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, PredefinedSplit\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "from sklearn import metrics   \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "import re\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "#warnings.filterwarnings('always')\n",
    "import pickle\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from imblearn.metrics import specificity_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import make_scorer, f1_score, roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn import metrics   \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool, cv\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "#from pandas_ml import ConfusionMatrix\n",
    "\n",
    "#import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(p): # \n",
    "    #Read input file of each person\n",
    "    filename='data/NOv_w5_emotionLabel_SelFeat_p'+str(p)+'.csv'\n",
    "    \n",
    "    raw_df= pd.read_csv(filename)\n",
    "    print(\"The shape of the dataframe is \",raw_df.shape)\n",
    "\n",
    "    return raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NANs with -999\n",
    "def prep_data(data):\n",
    "    return data.fillna(-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns\n",
    "def drop_cols(data, col_list):\n",
    "    return data.drop(col_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data with minmax\n",
    "def scale_data(trn_x, tst_x):\n",
    "    \n",
    "    sc= StandardScaler()\n",
    "    scaled_trn_x = sc.fit_transform(trn_x)\n",
    "    scaled_tst_x = sc.fit_transform(tst_x)\n",
    "    \n",
    "    return scaled_trn_x, scaled_tst_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling with SMOTE with 'minority' and 'not majority'\n",
    "def over_sample_SMOTE(X_train, y_train):\n",
    "    sm=SMOTE(sampling_strategy='not majority', random_state=10) # 'minority'\n",
    "    X_train_ovr, y_train_ovr=sm.fit_sample(X_train, y_train)\n",
    "\n",
    "    #print(X_train_ovr.shape, y_train_ovr.shape)\n",
    "    return X_train_ovr, y_train_ovr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling with SMOTENC with 'minority' and 'not majority'\n",
    "def over_sample_SMOTENC(X_train, y_train):\n",
    "    \n",
    "    sm = SMOTENC(sampling_strategy='not majority',random_state=10)\n",
    "    \n",
    "    #sm = SMOTENC(sampling_strategy='minority',random_state=10)\n",
    "    \n",
    "    X_train_ovr, y_train_ovr=sm.fit_sample(X_train, y_train)\n",
    "\n",
    "    #print(X_train_ovr.shape, y_train_ovr.shape)\n",
    "    return X_train_ovr, y_train_ovr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling with SVMSMOTE \n",
    "def over_sample_SVMSMOTE(X_train, y_train):\n",
    "    sm=SVMSMOTE(random_state=10)\n",
    "    \n",
    "    X_train_ovr, y_train_ovr=sm.fit_sample(X_train, y_train)\n",
    "\n",
    "    #print(X_train_ovr.shape, y_train_ovr.shape)\n",
    "    return X_train_ovr, y_train_ovr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataframes(p_list):\n",
    "    df = pd.DataFrame()\n",
    "    for p in p_list:\n",
    "        new_df = read_input(p)\n",
    "        df=df.append(new_df,ignore_index = True)\n",
    "        \n",
    "    #drop all variables that contain all NANs\n",
    "    df.dropna(axis=1,how='all', inplace=True)\n",
    "    #reset the index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    #drop columns with all zeros in pandas dataframe\n",
    "    df=df.T[(df!=0).any()].T\n",
    "    \n",
    "    #keep columns with missing values < 30%  \n",
    "    df = df.loc[:, df.isnull().mean() < .3]\n",
    "    \n",
    "    print(\"The shape of the merged dataframe is \",df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all columns that contain location information (if any)\n",
    "def drop_location(df):\n",
    "    print(df.shape)\n",
    "    df = df[df.columns.drop(list(df.filter(regex='location')))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex='latitude')))]\n",
    "    df = df[df.columns.drop(list(df.filter(regex='lonitude')))]\n",
    "    print(df.shape)\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_k_features(X_train_scaled,X_test_scaled,y_train,k):\n",
    "    selection = SelectKBest(mutual_info_classif, k)\n",
    "    X_train = selection.fit_transform(X_train_scaled,y_train)\n",
    "    X_test = selection.transform(X_test_scaled)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(accu, bl_accu, prec, rec_, spec_, roc_, f1_):   \n",
    "    print('.....................')\n",
    "    print(\"Average Accuracy: %.2f%% (%.2f)\" % (np.mean(accu), np.std(accu)))\n",
    "    print(\"Average Balanced_accuracy: %.2f%% (%.2f)\" % (np.mean(bl_accu),np.std(bl_accu)))\n",
    "    print(\"Average Precision: %.2f%% (%.2f)\" % (np.mean(prec),np.std(prec)))\n",
    "    print(\"Average Recall: %.2f%% (%.2f)\" % (np.mean(rec_),np.std(rec_)))\n",
    "    print(\"Average Specificity: %.2f%% (%.2f)\" % (np.mean(spec_),np.std(spec_)))\n",
    "    print(\"Average ROC AUC: %.2f%% (%.2f)\" % (np.mean(roc_),np.std(roc_)))\n",
    "    print(\"Average F1 score: %.2f%% (%.2f)\" % (np.mean(f1_),np.std(f1_)))\n",
    "    print('..................................................')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()), # MinMaxScaler()\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k=90)), #\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "search_space = [{'selector__k': [ 50, 70, 90]},\n",
    "                \n",
    "                {'classifier': [LogisticRegression(solver='lbfgs')],\n",
    "                 'classifier__C': [0.01, 0.1, 1.0],\n",
    "                 'classifier__penalty': ['l1', 'l2', None],\n",
    "                 'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                 'classifier__max_iter':[100, 150, 200], \n",
    "                 'classifier__class_weight':[None, 'balanced']},\n",
    "                 \n",
    "                {'classifier': [RandomForestClassifier()],\n",
    "                 'classifier__max_depth': [5, 10, 30, None],\n",
    "                 'classifier__criterion':['gini','entropy'], \n",
    "                 'classifier__bootstrap': [True],\n",
    "                 'classifier__max_features':['log2', None],\n",
    "                 'classifier__n_estimators': [50, 100, 200, 300, 400]},\n",
    "                \n",
    "                {'classifier': [MLPClassifier(random_state=1, early_stopping=True)],\n",
    "                 'classifier__hidden_layer_sizes' : [(50, 50, 50), (50, 100, 50), (20, 20, 20), (30, ), (50,),(100,)], \n",
    "                 'classifier__activation' : ['tanh', 'relu', 'logistic'],\n",
    "                 'classifier__max_iter':[50, 100, 150, 200, 300],\n",
    "                 'classifier__solver': ['sgd', 'adam', 'lbfgs'],\n",
    "                 'classifier__alpha': [0.0001, 0.001, 0.05]},\n",
    "                \n",
    "                {'classifier': [CatBoostClassifier(random_seed=1)],\n",
    "                 'classifier__learning_rate': [0.05, 0.1, 0.15, 0.2]},\n",
    "                \n",
    "                {'classifier': [XGBClassifier(random_state=1)],\n",
    "                 'classifier__learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
    "                 'classifier__colsample_bytree':[.5, .75, 1],\n",
    "                 'classifier__max_depth': np.arange(3, 6, 10),\n",
    "                 'classifier__n_estimators': [50, 100, 200, 300, 400]}]\n",
    "                  \n",
    "     \n",
    "scorers = {\n",
    "    'precision_score': make_scorer(precision_score, average='macro'),\n",
    "    'recall_score': make_scorer(recall_score, average='macro'),\n",
    "    'accuracy_score': make_scorer(accuracy_score, average='macro')\n",
    "}\n",
    "\n",
    "scorer = make_scorer(f1_score, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_pipe = Pipeline([('scaler', StandardScaler()), # MinMaxScaler()\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k=90)), #\n",
    "                 ('classifier', LogisticRegression())])\n",
    "\n",
    "LR_search_space = [{'selector__k': [ 50, 70, 90, 110]},\n",
    "                \n",
    "                {'classifier': [LogisticRegression(solver='lbfgs')],\n",
    "                 'classifier__C': [0.01, 0.1, 1.0],\n",
    "                 'classifier__penalty': ['l1', 'l2', None],\n",
    "                 'classifier__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "                 'classifier__max_iter':[100, 150, 200], \n",
    "                 'classifier__class_weight':[None, 'balanced']}]\n",
    "                 \n",
    "################################################################################          \n",
    "\n",
    "RF_pipe = Pipeline([('scaler', StandardScaler()), # MinMaxScaler()\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k=90)), #\n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "RF_search_space = [{'selector__k': [ 50, 70, 90, 110]},\n",
    "                \n",
    "                {'classifier': [RandomForestClassifier()],\n",
    "                 'classifier__max_depth': [5, 10, 30, None],\n",
    "                 'classifier__criterion':['gini','entropy'], \n",
    "                 'classifier__bootstrap': [True],\n",
    "                 'classifier__max_features':['log2', None],\n",
    "                 'classifier__n_estimators': [50, 100, 200, 300, 400]}]\n",
    "                  \n",
    "################################################################################\n",
    "\n",
    "MLP_pipe = Pipeline([('scaler', StandardScaler()), # MinMaxScaler()\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k=90)), #\n",
    "                 ('classifier', MLPClassifier(random_state=1, early_stopping=True))])\n",
    "\n",
    "MLP_search_space = [{'selector__k': [ 50, 70, 90, 110]},\n",
    "                \n",
    "                {'classifier': [MLPClassifier(random_state=1, early_stopping=True)],\n",
    "                 'classifier__hidden_layer_sizes' : [(50, 50, 50), (50, 100, 50), (20, 20, 20), (30, ), (50,),(100,)], \n",
    "                 'classifier__activation' : ['tanh', 'relu', 'logistic'],\n",
    "                 'classifier__max_iter':[50, 100, 150, 200, 300],\n",
    "                 'classifier__solver': ['sgd', 'adam', 'lbfgs'],\n",
    "                 'classifier__alpha': [0.0001, 0.001, 0.05]}]\n",
    "\n",
    "################################################################################\n",
    "\n",
    "CB_pipe = Pipeline([('scaler', StandardScaler()), # MinMaxScaler()\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k=90)), #\n",
    "                 ('classifier', CatBoostClassifier(random_seed=1))])\n",
    "\n",
    "CB_search_space = [{'selector__k': [ 50, 70, 90, 110]},\n",
    "                \n",
    "                {'classifier': [CatBoostClassifier(random_seed=1, verbose=False)],\n",
    "                 'classifier__learning_rate': [0.05, 0.1, 0.15, 0.2]}]\n",
    "#'iterations': Integer(10, 1000),\n",
    " #                'depth': Integer(1, 8),\n",
    "  #               'learning_rate': Real(0.01, 1.0, 'log-uniform'),\n",
    "   #              'random_strength': Real(1e-9, 10, 'log-uniform'),\n",
    "    #             'bagging_temperature': Real(0.0, 1.0),\n",
    "     #            'border_count': Integer(1, 255),\n",
    "      #           'l2_leaf_reg': Integer(2, 30),\n",
    "       #          'scale_pos_weight':Real(0.01, 1.0, 'uniform')\n",
    "\n",
    "################################################################################\n",
    "\n",
    "XGB_pipe = Pipeline([('scaler', StandardScaler()), # MinMaxScaler()\n",
    "                 ('selector', SelectKBest(mutual_info_classif, k=90)), #\n",
    "                 ('classifier', XGBClassifier(random_state=1))])\n",
    "\n",
    "XGB_search_space = [{'selector__k': [ 50, 70, 90, 110]},\n",
    "                \n",
    "                {'classifier': [XGBClassifier(random_state=1)],\n",
    "                 'classifier__learning_rate': [0.05, 0.1, 0.15, 0.2],\n",
    "                 'classifier__colsample_bytree':[.5, .75, 1],\n",
    "                 'classifier__max_depth': np.arange(3, 6, 10),\n",
    "                 'classifier__n_estimators': [50, 100, 200, 300, 400]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_list=[8,10,12,13,15,20,21,25, 27, 33,35,40,46,48,49,52,54,55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe is  (269, 274)\n",
      "The shape of the dataframe is  (1276, 274)\n",
      "The shape of the dataframe is  (540, 274)\n",
      "The shape of the merged dataframe is  (2085, 233)\n",
      "The shape of the dataframe is  (1246, 274)\n",
      "The shape of the dataframe is  (552, 274)\n",
      "The shape of the dataframe is  (624, 274)\n",
      "The shape of the merged dataframe is  (2422, 238)\n",
      "The shape of the dataframe is  (1269, 274)\n",
      "The shape of the dataframe is  (258, 274)\n",
      "The shape of the dataframe is  (318, 274)\n",
      "The shape of the merged dataframe is  (1845, 179)\n",
      "The shape of the dataframe is  (863, 274)\n",
      "The shape of the dataframe is  (912, 274)\n",
      "The shape of the dataframe is  (753, 274)\n",
      "The shape of the merged dataframe is  (2528, 237)\n",
      "The shape of the dataframe is  (756, 274)\n",
      "The shape of the dataframe is  (1165, 274)\n",
      "The shape of the dataframe is  (693, 274)\n",
      "The shape of the merged dataframe is  (2614, 186)\n",
      "The shape of the dataframe is  (869, 274)\n",
      "The shape of the dataframe is  (530, 274)\n",
      "The shape of the dataframe is  (708, 274)\n",
      "The shape of the merged dataframe is  (2107, 189)\n"
     ]
    }
   ],
   "source": [
    "# make a predifined CV split (test_fold)\n",
    "test_fold = []\n",
    "for i in range(nfolds):\n",
    "    p_test = p_list[i*3:i*3+3]\n",
    "    \n",
    "    df_test = merge_dataframes(p_test)\n",
    "    \n",
    "    tst = [i] * df_test.shape[0] \n",
    "    \n",
    "    test_fold= test_fold + tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe is  (269, 274)\n",
      "The shape of the dataframe is  (1276, 274)\n",
      "The shape of the dataframe is  (540, 274)\n",
      "The shape of the dataframe is  (1246, 274)\n",
      "The shape of the dataframe is  (552, 274)\n",
      "The shape of the dataframe is  (624, 274)\n",
      "The shape of the dataframe is  (1269, 274)\n",
      "The shape of the dataframe is  (258, 274)\n",
      "The shape of the dataframe is  (318, 274)\n",
      "The shape of the dataframe is  (863, 274)\n",
      "The shape of the dataframe is  (912, 274)\n",
      "The shape of the dataframe is  (753, 274)\n",
      "The shape of the dataframe is  (756, 274)\n",
      "The shape of the dataframe is  (1165, 274)\n",
      "The shape of the dataframe is  (693, 274)\n",
      "The shape of the dataframe is  (869, 274)\n",
      "The shape of the dataframe is  (530, 274)\n",
      "The shape of the dataframe is  (708, 274)\n",
      "The shape of the merged dataframe is  (13601, 204)\n",
      "(13601, 204)\n",
      "(13601, 191)\n"
     ]
    }
   ],
   "source": [
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "# df contains all persons' data in one dataset\n",
    "df = merge_dataframes(p_list)\n",
    "df = prep_data(df)\n",
    "\n",
    "# remove day_of_month variable if present in data\n",
    "if 'day_of_month' in df.columns:\n",
    "    drop_col=['day_of_month']\n",
    "    df=drop_cols(df, drop_col)\n",
    "\n",
    "#drop all columns that contain location information \n",
    "df = drop_location(df)\n",
    "\n",
    "\n",
    "labels = list(df.columns)\n",
    "labels.remove('emotion')\n",
    "\n",
    "X = df[labels]\n",
    "y = df['emotion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_wrapper(pipe = pipe, search_space = search_space, verbose= False,refit_score=scorer):\n",
    "    \"\"\"\n",
    "    fits a GridSearchCV classifiers using refit_score for optimization\n",
    "    prints classifier performance metrics\n",
    "    \"\"\"\n",
    "    #cross_validation = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    cross_validation = ps\n",
    "    \n",
    "    grid_search = GridSearchCV(pipe, search_space, cv=cross_validation, verbose=verbose,  n_jobs = -1) #scoring=scorer, refit=scorer\n",
    "    \n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 84 candidates, totalling 504 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 26.3min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 84.0min\n",
      "[Parallel(n_jobs=-1)]: Done 504 out of 504 | elapsed: 165.1min finished\n"
     ]
    }
   ],
   "source": [
    "# do gird search for best parameters\n",
    "pipeline_grid_search_RF = grid_search_wrapper(pipe = RF_pipe, search_space = RF_search_space, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 64 candidates, totalling 384 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 35.3min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 106.6min\n",
      "[Parallel(n_jobs=-1)]: Done 384 out of 384 | elapsed: 120.3min finished\n"
     ]
    }
   ],
   "source": [
    "pipeline_grid_search_XGB = grid_search_wrapper(pipe = XGB_pipe, search_space = XGB_search_space, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 274 candidates, totalling 1644 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 21.8min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 39.6min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 63.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 100.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1644 out of 1644 | elapsed: 123.1min finished\n"
     ]
    }
   ],
   "source": [
    "pipeline_grid_search_LR = grid_search_wrapper(pipe = LR_pipe, search_space = LR_search_space, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 814 candidates, totalling 4884 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed: 49.2min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed: 76.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1442 tasks      | elapsed: 108.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1969 tasks      | elapsed: 145.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2576 tasks      | elapsed: 187.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3265 tasks      | elapsed: 237.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 294.2min\n",
      "[Parallel(n_jobs=-1)]: Done 4884 out of 4884 | elapsed: 353.3min finished\n"
     ]
    }
   ],
   "source": [
    "pipeline_grid_search_MLP = grid_search_wrapper(pipe = MLP_pipe, search_space = MLP_search_space, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_grid_search_CB = grid_search_wrapper(pipe = CB_pipe, search_space = CB_search_space, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('scaler',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('selector',\n",
      "                 SelectKBest(k=90,\n",
      "                             score_func=<function mutual_info_classif at 0x1a23e49830>)),\n",
      "                ('classifier',\n",
      "                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
      "                                        class_weight=None, criterion='gini',\n",
      "                                        max_depth=10, max_features='log2',\n",
      "                                        max_leaf_nodes=None, max_samples=None,\n",
      "                                        min_impurity_decrease=0.0,\n",
      "                                        min_impurity_split=None,\n",
      "                                        min_samples_leaf=1, min_samples_split=2,\n",
      "                                        min_weight_fraction_leaf=0.0,\n",
      "                                        n_estimators=50, n_jobs=None,\n",
      "                                        oob_score=False, random_state=None,\n",
      "                                        verbose=0, warm_start=False))],\n",
      "         verbose=False)\n",
      "0.4111463915151365\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_grid_search_RF.best_estimator_)\n",
    "print(pipeline_grid_search_RF.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('scaler',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('selector',\n",
      "                 SelectKBest(k=90,\n",
      "                             score_func=<function mutual_info_classif at 0x1a23e49830>)),\n",
      "                ('classifier',\n",
      "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                               colsample_bylevel=1, colsample_bynode=1,\n",
      "                               colsample_bytree=0.75, gamma=0,\n",
      "                               learning_rate=0.15, max_delta_step=0,\n",
      "                               max_depth=3, min_child_weight=1, missing=None,\n",
      "                               n_estimators=400, n_jobs=1, nthread=None,\n",
      "                               objective='multi:softprob', random_state=1,\n",
      "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "                               seed=None, silent=None, subsample=1,\n",
      "                               verbosity=1))],\n",
      "         verbose=False)\n",
      "0.4143208309782603\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_grid_search_XGB.best_estimator_)\n",
    "print(pipeline_grid_search_XGB.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('scaler',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('selector',\n",
      "                 SelectKBest(k=90,\n",
      "                             score_func=<function mutual_info_classif at 0x1a23e49830>)),\n",
      "                ('classifier',\n",
      "                 LogisticRegression(C=0.01, class_weight=None, dual=False,\n",
      "                                    fit_intercept=True, intercept_scaling=1,\n",
      "                                    l1_ratio=None, max_iter=200,\n",
      "                                    multi_class='auto', n_jobs=None,\n",
      "                                    penalty='l1', random_state=None,\n",
      "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
      "                                    warm_start=False))],\n",
      "         verbose=False)\n",
      "0.3995603428246966\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_grid_search_LR.best_estimator_)\n",
    "print(pipeline_grid_search_LR.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('scaler',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('selector',\n",
      "                 SelectKBest(k=90,\n",
      "                             score_func=<function mutual_info_classif at 0x1a23e49830>)),\n",
      "                ('classifier',\n",
      "                 <catboost.core.CatBoostClassifier object at 0x1a4f9c2110>)],\n",
      "         verbose=False)\n",
      "0.40175145640320165\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_grid_search_CB.best_estimator_)\n",
    "print(pipeline_grid_search_CB.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('scaler',\n",
      "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
      "                ('selector',\n",
      "                 SelectKBest(k=90,\n",
      "                             score_func=<function mutual_info_classif at 0x1a23e49830>)),\n",
      "                ('classifier',\n",
      "                 MLPClassifier(activation='logistic', alpha=0.0001,\n",
      "                               batch_size='auto', beta_1=0.9, beta_2=0.999,\n",
      "                               early_stopping=True, epsilon=1e-08,\n",
      "                               hidden_layer_sizes=(50, 50, 50),\n",
      "                               learning_rate='constant',\n",
      "                               learning_rate_init=0.001, max_fun=15000,\n",
      "                               max_iter=50, momentum=0.9, n_iter_no_change=10,\n",
      "                               nesterovs_momentum=True, power_t=0.5,\n",
      "                               random_state=1, shuffle=True, solver='sgd',\n",
      "                               tol=0.0001, validation_fraction=0.1,\n",
      "                               verbose=False, warm_start=False))],\n",
      "         verbose=False)\n",
      "0.448384439129946\n"
     ]
    }
   ],
   "source": [
    "print(pipeline_grid_search_MLP.best_estimator_)\n",
    "print(pipeline_grid_search_MLP.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best models\n",
    "\n",
    "LR_model = LogisticRegression(C=0.01, class_weight=None, dual=False,\n",
    "                                    fit_intercept=True, intercept_scaling=1,\n",
    "                                    l1_ratio=None, max_iter=200,\n",
    "                                    multi_class='auto', n_jobs=None,\n",
    "                                    penalty='l1', random_state=None,\n",
    "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
    "                                    warm_start=False)\n",
    "\n",
    "RF_model = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
    "                                        class_weight=None, criterion='gini',\n",
    "                                        max_depth=10, max_features='log2',\n",
    "                                        max_leaf_nodes=None, max_samples=None,\n",
    "                                        min_impurity_decrease=0.0,\n",
    "                                        min_impurity_split=None,\n",
    "                                        min_samples_leaf=1, min_samples_split=2,\n",
    "                                        min_weight_fraction_leaf=0.0,\n",
    "                                        n_estimators=50, n_jobs=None,\n",
    "                                        oob_score=False, random_state=None,\n",
    "                                        verbose=0, warm_start=False)\n",
    "\n",
    "MLP_model = MLPClassifier(activation='logistic', alpha=0.0001,\n",
    "                               batch_size='auto', beta_1=0.9, beta_2=0.999,\n",
    "                               early_stopping=True, epsilon=1e-08,\n",
    "                               hidden_layer_sizes=(50, 50, 50),\n",
    "                               learning_rate='constant',\n",
    "                               learning_rate_init=0.001, max_fun=15000,\n",
    "                               max_iter=50, momentum=0.9, n_iter_no_change=10,\n",
    "                               nesterovs_momentum=True, power_t=0.5,\n",
    "                               random_state=1, shuffle=True, solver='sgd',\n",
    "                               tol=0.0001, validation_fraction=0.1,\n",
    "                               verbose=False, warm_start=False)\n",
    "\n",
    "XGB_model = XGBClassifier(base_score=0.5, booster='gbtree',\n",
    "                               colsample_bylevel=1, colsample_bynode=1,\n",
    "                               colsample_bytree=0.75, gamma=0,\n",
    "                               learning_rate=0.15, max_delta_step=0,\n",
    "                               max_depth=3, min_child_weight=1, missing=None,\n",
    "                               n_estimators=400, n_jobs=1, nthread=None,\n",
    "                               objective='multi:softprob', random_state=1,\n",
    "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "                               seed=None, silent=None, subsample=1,\n",
    "                               verbosity=1)\n",
    "\n",
    "\n",
    "CB_model = CatBoostClassifier(random_seed=1, verbose=False,learning_rate= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {} # dictionary of best models with best parameters\n",
    "\n",
    "best_models['Logistic Regression'] = LR_model\n",
    "best_models['RandomForest Classifier'] = RF_model\n",
    "best_models['MLP Classifier'] = MLP_model\n",
    "best_models['XGBoost Classifier'] = XGB_model\n",
    "best_models['CatBoost Classifier'] = CB_model\n",
    "\n",
    "n_features = [90, 90, 90, 90, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfolds = 6\n",
    "rnd_state=42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/majed_al-jefri/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1859: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/Users/majed_al-jefri/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/majed_al-jefri/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1859: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/Users/majed_al-jefri/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restuls for:  Logistic Regression\n",
      ".....................\n",
      "Average Accuracy: 32.61% (2.65)\n",
      "Average Balanced_accuracy: 30.66% (8.82)\n",
      "Average Precision: 26.48% (2.32)\n",
      "Average Recall: 28.27% (7.76)\n",
      "Average Specificity: 82.93% (0.76)\n",
      "Average ROC AUC: 60.55% (3.41)\n",
      "Average F1 score: 23.04% (2.82)\n",
      "..................................................\n",
      "\n",
      "\n",
      "Restuls for:  RandomForest Classifier\n",
      ".....................\n",
      "Average Accuracy: 38.99% (8.71)\n",
      "Average Balanced_accuracy: 23.27% (2.43)\n",
      "Average Precision: 25.33% (2.20)\n",
      "Average Recall: 25.94% (2.08)\n",
      "Average Specificity: 76.93% (2.63)\n",
      "Average ROC AUC: 55.43% (4.37)\n",
      "Average F1 score: 21.81% (3.24)\n",
      "..................................................\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/majed_al-jefri/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1859: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n",
      "/Users/majed_al-jefri/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restuls for:  MLP Classifier\n",
      ".....................\n",
      "Average Accuracy: 34.24% (17.81)\n",
      "Average Balanced_accuracy: 17.85% (4.63)\n",
      "Average Precision: 29.04% (15.81)\n",
      "Average Recall: 45.56% (26.76)\n",
      "Average Specificity: 47.74% (22.16)\n",
      "Average ROC AUC: 49.46% (7.23)\n",
      "Average F1 score: 32.78% (20.85)\n",
      "..................................................\n",
      "\n",
      "\n",
      "Restuls for:  XGBoost Classifier\n",
      ".....................\n",
      "Average Accuracy: 29.63% (15.13)\n",
      "Average Balanced_accuracy: 24.17% (4.63)\n",
      "Average Precision: 25.15% (5.78)\n",
      "Average Recall: 25.40% (6.51)\n",
      "Average Specificity: 78.49% (1.52)\n",
      "Average ROC AUC: 54.10% (6.63)\n",
      "Average F1 score: 19.06% (6.91)\n",
      "..................................................\n",
      "\n",
      "\n",
      "Restuls for:  CatBoost Classifier\n",
      ".....................\n",
      "Average Accuracy: 39.52% (13.24)\n",
      "Average Balanced_accuracy: 23.32% (2.91)\n",
      "Average Precision: 27.18% (10.23)\n",
      "Average Recall: 27.81% (5.77)\n",
      "Average Specificity: 75.36% (4.66)\n",
      "Average ROC AUC: 56.83% (6.14)\n",
      "Average F1 score: 21.20% (6.95)\n",
      "..................................................\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is to get all the detailed performance meterics after selecting the best model parameters\n",
    "k_i = -1    \n",
    "for model_name, model in best_models.items(): \n",
    "    k_i = k_i + 1\n",
    "    accu = []\n",
    "    prec = []\n",
    "    rec_ = []\n",
    "    f1_ = []\n",
    "    bl_accu = []\n",
    "    roc_ = []\n",
    "    spec_ = []\n",
    "\n",
    "    i = 1\n",
    "    for train_index, test_index in ps.split():\n",
    "        #print(\"fold\", i)\n",
    "        i+=1\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index] \n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        #scale features\n",
    "        X_train_scaled, X_test_scaled= scale_data(X_train, X_test) \n",
    "        #feature selection\n",
    "        X_train, X_test = select_k_features(X_train_scaled,X_test_scaled,y_train,k=n_features[k_i])\n",
    "\n",
    "        #oversample training data\n",
    "        #X_train_imb,y_train_imb=over_sample_SMOTE(X_train, y_train)\n",
    "        #X_train_imb,y_train_imb=over_sample_SMOTENC(X_train, y_train)\n",
    "        X_train_imb,y_train_imb=over_sample_SVMSMOTE(X_train, y_train)\n",
    "\n",
    "\n",
    "        # train model on imbalance-handled data\n",
    "        model.fit(X_train_imb, y_train_imb)\n",
    "\n",
    "        #train model on imbalance data \n",
    "        #model.fit(X_train, y_train)\n",
    "\n",
    "        # test model, measure class label and probability score\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_scores = model.predict_proba(X_test)\n",
    "\n",
    "        #calculate metrices\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        bl_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        precision=precision_score(y_test, y_pred,  average='macro',labels=np.unique(y_pred)) #'weighted', 'micro', 'micro'\n",
    "        recall=recall_score(y_test, y_pred,  average='macro',labels=np.unique(y_pred))\n",
    "        #kappa=cohen_kappa_score(y_pred, y_test)\n",
    "        spec=specificity_score(y_test, y_pred, average='macro',labels=np.unique(y_pred))\n",
    "        #roc=roc_auc_score(y_test, y_scores, multi_class='ovr', average='macro')\n",
    "        f1=f1_score(y_test, y_pred,  average='macro',labels=np.unique(y_pred))\n",
    "\n",
    "        # sometimes not all classes are present in the test set\n",
    "        not_present = list(set(model.classes_)-set(y_test.unique()))\n",
    "        # get that class\n",
    "        if not_present:\n",
    "            not_present=not_present[0] # get the element then its index\n",
    "            ind= list(model.classes_).index(not_present)\n",
    "            y_scores = np.delete(y_scores,ind,1) # delete it from the scores\n",
    "            y_scores = y_scores / y_scores.sum(axis=1)[:,None]  #make sure sum equals ro 0 (sum of probabilities)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "        roc=roc_auc_score(y_test, y_scores, multi_class='ovr', average='macro')\n",
    "\n",
    "        ac=accuracy * 100.0\n",
    "        pr=precision*100\n",
    "        rc=recall*100\n",
    "        f1_p=f1*100\n",
    "        bl_ac=bl_accuracy*100\n",
    "        roc=roc*100\n",
    "        spec=spec*100\n",
    "    \n",
    "        accu.append(ac)\n",
    "        prec.append(pr)\n",
    "        rec_.append(rc)\n",
    "        f1_.append(f1_p)\n",
    "        bl_accu.append(bl_ac)\n",
    "        roc_.append(roc)\n",
    "        spec_.append(spec)\n",
    "    \n",
    "    print('Restuls for: ', model_name)\n",
    "    print_results(accu, bl_accu, prec, rec_, spec_, roc_, f1_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
